{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qm8x2zhhd-8"
      },
      "source": [
        "# Integrating tools/function with LLM using Azure OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu6uxcLphsRz",
        "outputId": "dcee60be-8dfc-4600-e9e7-a89b5f7b163d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.1/324.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install openai --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GTT0Jc-qhVby"
      },
      "outputs": [],
      "source": [
        "# Define constants for the Azure OpenAI API configuration\n",
        "api_key = \"xxxxxxxxxxxxxxxxx\"\n",
        "api_version = \"2023-07-01-preview\" # \"2023-05-15\"\n",
        "azure_endpoint = \"https://xxxxxxxxxx.openai.azure.com/\"\n",
        "model_name = \"gpt-35-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PLKRALkuhVby"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import requests\n",
        "\n",
        "client = AzureOpenAI(\n",
        "  azure_endpoint = azure_endpoint, \n",
        "  api_key=api_key,  \n",
        "  api_version=api_version\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMs1SliPer2Q"
      },
      "source": [
        "In an API call, you can describe functions and have the model intelligently choose to output a JSON object containing arguments to call one or many functions. The Chat Completions API does not call the function; instead, the model generates JSON that you can use to call the function in your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6voT6dEZOEQ"
      },
      "source": [
        "## Supported models\n",
        "Not all model versions are trained with function calling data. Function calling is supported with the following models: gpt-4, gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-0613, gpt-3.5-turbo, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, and gpt-3.5-turbo-0613\n",
        "\n",
        "In addition, parallel function calls is supported on the following models: gpt-4-turbo-preview, gpt-4-0125-preview, gpt-4-1106-preview, gpt-3.5-turbo-0125, and gpt-3.5-turbo-1106"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FUAVQTH1FCob"
      },
      "outputs": [],
      "source": [
        "# Define the OpenWeatherMap API key\n",
        "OWM_API_KEY = \"29af1cea50a401d8e624eea4660b3f59\"\n",
        "\n",
        "def get_current_weather(location, unit=\"kelvin\"):\n",
        "    \"\"\"\n",
        "    Fetches the current weather information for a given location.\n",
        "\n",
        "    Parameters:\n",
        "    - location: str, the name of the location (e.g., \"Paris\").\n",
        "    - unit: str, the unit of temperature (default is \"kelvin\").\n",
        "\n",
        "    Returns:\n",
        "    - str: JSON formatted string containing weather information.\n",
        "    \"\"\"\n",
        "    # Construct the API request URL\n",
        "    url = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={OWM_API_KEY}\"\n",
        "    \n",
        "    # Send the API request\n",
        "    response = requests.get(url)\n",
        "    \n",
        "    # Parse the temperature and weather forecast from the response\n",
        "    temp = response.json()['main']['temp']\n",
        "    forecast = [response.json()['weather'][0]['main'], response.json()['weather'][0]['description']]\n",
        "\n",
        "    # Create a dictionary with the weather information\n",
        "    weather_info = {\n",
        "        \"location\": location,\n",
        "        \"temperature\": temp,\n",
        "        \"unit\": 'Kelvin',\n",
        "        \"forecast\": forecast\n",
        "    }\n",
        "    \n",
        "    # Return the weather information as a JSON string\n",
        "    return json.dumps(weather_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zvpewsATGB30",
        "outputId": "01d29cea-3467-4fc5-b124-8f38b473468f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"location\": \"Paris\", \"temperature\": 293.84, \"unit\": \"Kelvin\", \"forecast\": [\"Clear\", \"clear sky\"]}'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_current_weather(\"Paris\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sTPInk2CiVpV",
        "outputId": "bbd31641-53d2-4f91-d578-9d5ec50c4e03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'{\"location\": \"Manila\", \"temperature\": 302.84, \"unit\": \"Kelvin\", \"forecast\": [\"Clouds\", \"scattered clouds\"]}'"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_current_weather(\"Manila\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OLj1VFabUCR",
        "outputId": "cb038e0a-fc95-4c8f-9f14-f783a84f494e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-9ZgZgS218VdSSMvUsCS9AxAUEAMXq\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"logprobs\": null,\n",
            "      \"message\": {\n",
            "        \"content\": \"AI (Artificial Intelligence) is a branch of computer science that focuses on creating smart machines capable of performing tasks that typically require human intelligence. AI systems are designed to simulate human intelligence, including the ability to learn, reason, solve problems, understand language, and perceive and respond to their environments. AI technologies can be categorized into narrow AI, which is designed for specific tasks like facial recognition or language translation, and general AI, which aims to possess the same intellectual capabilities as a human.\",\n",
            "        \"role\": \"assistant\",\n",
            "        \"function_call\": null,\n",
            "        \"tool_calls\": null\n",
            "      },\n",
            "      \"content_filter_results\": {\n",
            "        \"hate\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"self_harm\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"sexual\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"violence\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1718292164,\n",
            "  \"model\": \"gpt-35-turbo\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"system_fingerprint\": null,\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 97,\n",
            "    \"prompt_tokens\": 11,\n",
            "    \"total_tokens\": 108\n",
            "  },\n",
            "  \"prompt_filter_results\": [\n",
            "    {\n",
            "      \"prompt_index\": 0,\n",
            "      \"content_filter_results\": {\n",
            "        \"hate\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"self_harm\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"sexual\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        },\n",
            "        \"violence\": {\n",
            "          \"filtered\": false,\n",
            "          \"severity\": \"safe\"\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\":\"user\",'content':\"what is AI?\"}]\n",
        "results = client.chat.completions.create(model = model_name, messages = messages)\n",
        "print(results.model_dump_json(indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Define tools/functions for the LLM to use\n",
        "tools = [\n",
        "        {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\n",
        "                \"name\": \"get_current_weather\",\n",
        "                \"description\": \"Get the current weather in a given location\",\n",
        "                \"parameters\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"properties\": {\n",
        "                        \"location\": {\n",
        "                            \"type\": \"string\",\n",
        "                            \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                        },\n",
        "                        \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\",\"kelvin\"]},\n",
        "                    },\n",
        "                    \"required\": [\"location\"],\n",
        "                },\n",
        "            },\n",
        "        }\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "9gdvGUJGbheA"
      },
      "outputs": [],
      "source": [
        "def get_response(messages, tools, model=model_name):\n",
        "    \"\"\"\n",
        "    Handles interaction with the LLM, including making function calls if needed.\n",
        "\n",
        "    Parameters:\n",
        "    - messages: list, the conversation history.\n",
        "    - tools: list, the functions/tools available to the LLM.\n",
        "    - model: str, the model name to use (default is model_name).\n",
        "\n",
        "    Returns:\n",
        "    - str: The response from the LLM or function.\n",
        "    \"\"\"\n",
        "    # Create a chat completion with the given messages and tools\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        tools=tools,\n",
        "        tool_choice='auto',\n",
        "        temperature=0.2,\n",
        "    )\n",
        "\n",
        "    # Get the generated response and tool calls (if any)\n",
        "    response = response.choices[0].message\n",
        "    tool_calls = response.tool_calls\n",
        "\n",
        "    try:\n",
        "        if tool_calls:\n",
        "            print(\"Making a function call\")\n",
        "            # Available functions\n",
        "            available_functions = {\n",
        "                \"get_current_weather\": get_current_weather,\n",
        "            }\n",
        "            print(tool_calls)\n",
        "\n",
        "            # Extend conversation with assistant's reply\n",
        "            messages.append(response)\n",
        "\n",
        "            # Handle each function call\n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = available_functions[function_name]\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "                function_response = function_to_call(\n",
        "                    location=function_args.get(\"location\"),\n",
        "                    unit=function_args.get(\"unit\"),\n",
        "                )\n",
        "                messages.append(\n",
        "                    {\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"role\": \"tool\",\n",
        "                        \"name\": function_name,\n",
        "                        \"content\": function_response,\n",
        "                    }\n",
        "                )\n",
        "\n",
        "            # Get a new response from the model with the function response\n",
        "            second_response = client.chat.completions.create(\n",
        "                model=model_name,\n",
        "                messages=messages,\n",
        "            )\n",
        "            return second_response\n",
        "        else:\n",
        "            return response.content\n",
        "    except Exception as e:\n",
        "        print(\"Error occurred\", e)\n",
        "        return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWL0wVqFdeBm",
        "outputId": "af1791a3-7e05-4527-e75e-9caefe8bd685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AI, or Artificial Intelligence, is a branch of computer science that focuses on creating intelligent machines that can perform tasks that would typically require human intelligence. It involves the development of algorithms and models that enable machines to learn, reason, and make decisions.\n"
          ]
        }
      ],
      "source": [
        "# Example usage of get_response function\n",
        "messages = [{\"role\": \"user\", \"content\": \"Provide a 2 line explanation for AI\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0H4DaivdmPW",
        "outputId": "67d2c2f0-8be3-41c2-aee4-7f115fadf366"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Making a function call\n",
            "[ChatCompletionMessageToolCall(id='call_7laQPyXq7XwZmqbit3kwYaS9', function=Function(arguments='{\\n  \"location\": \"Tokyo\"\\n}', name='get_current_weather'), type='function')]\n",
            "The weather in Tokyo today is clear with a temperature of 295.26 Kelvin.\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"user\", \"content\": \"How is the weather in Tokyo today?\"}]\n",
        "response = get_response(messages, tools)\n",
        "print(response.choices[0].message.content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
